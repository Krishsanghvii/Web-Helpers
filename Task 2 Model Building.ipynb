{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqDKakn6KeSYgmwqpqofPx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","import string\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","# Initialize lists to hold the labels and text data\n","labels = []\n","text = []\n","\n","# Open the file and read each line\n","with open('/notebooks/train.ft.txt.bz2', 'r') as f:\n","    for line in f:\n","        # The label is the part between '_label_' and the first space\n","        labels.append(int(line.split(' ')[0].replace('_label_', '')))\n","\n","        # The text is the part after the first space\n","        text.append(line.split(' ', 1)[1].rstrip())\n","\n","# Create a DataFrame from the labels and text\n","df = pd.DataFrame(list(zip(labels, text)), columns=['label', 'text'])\n","\n","# Download necessary NLTK data\n","nltk.download(['punkt', 'wordnet', 'stopwords'])\n","\n","# Initialize a WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Define the English stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess_text(text):\n","    # Convert the text to lower case\n","    text = text.lower()\n","\n","    # Remove punctuation\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","    # Tokenize the text\n","    word_tokens = word_tokenize(text)\n","\n","    # Remove stopwords and lemmatize the words\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokens if word not in stop_words]\n","\n","    # Join the words back into a single string\n","    text = ' '.join(lemmatized_words)\n","\n","    return text\n","\n","# Apply the preprocessing to the 'text' column of the DataFrame\n","df['text'] = df['text'].apply(preprocess_text)\n","\n","# Subtract 1 from the labels to make them 0 (for negative) and 1 (for positive)\n","df['label'] = df['label'] - 1\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n","\n","# Initialize a tokenizer\n","tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n","tokenizer.fit_on_texts(X_train)\n","\n","# Tokenize the data\n","X_train_sequences = tokenizer.texts_to_sequences(X_train)\n","X_test_sequences = tokenizer.texts_to_sequences(X_test)   500x500 , 100x100\n","\n","# Pad the sequences\n","X_train_padded = pad_sequences(X_train_sequences, padding='post')\n","X_test_padded = pad_sequences(X_test_sequences, padding='post', maxlen=X_train_padded.shape[1])\n","\n","# Define the model\n","model = Sequential([\n","    Embedding(10000, 16, input_length=X_train_padded.shape[1]),\n","    Conv1D(128, 5, activation='relu'),\n","    GlobalMaxPooling1D(),\n","    Dense(2, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(X_train_padded, y_train, validation_data=(X_test_padded, y_test), epochs=10)\n","\n","# Load the test data\n","labels_test = []\n","text_test = []\n","\n","with open('/notebooks/test.ft.txt.bz2', 'r') as f:\n","    for line in f:\n","        labels_test.append(int(line.split(' ')[0].replace('_label_', '')))\n","        text_test.append(line.split(' ', 1)[1].rstrip())\n","\n","# Create a DataFrame from the labels and text\n","df_test = pd.DataFrame(list(zip(labels_test, text_test)), columns=['label', 'text'])\n","\n","# Preprocess the text\n","df_test['text'] = df_test['text'].apply(preprocess_text)\n","\n","# Subtract 1 from the labels to make them 0 (for negative) and 1 (for positive)\n","df_test['label'] = df_test['label'] - 1\n","\n","# Tokenize and pad the sequences\n","X_test_sequences = tokenizer.texts_to_sequences(df_test['text'])\n","X_test_padded = pad_sequences(X_test_sequences, padding='post', maxlen=X_train_padded.shape[1])\n","\n","# Evaluate the model on the test data\n","loss, accuracy = model.evaluate(X_test_padded, df_test['label'])\n","\n","print('Test Loss:', loss)\n","print('Test Accuracy:', accuracy)"],"metadata":{"id":"viOyCL0PU4mk","colab":{"base_uri":"https://localhost:8080/","height":130},"executionInfo":{"status":"error","timestamp":1690267042715,"user_tz":-330,"elapsed":1029,"user":{"displayName":"Krish Sanghvi","userId":"04599451902355984155"}},"outputId":"a09a25c7-7bf7-4ee8-b702-0460aa8f20da"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-2e6f8a00a0a6>\"\u001b[0;36m, line \u001b[0;32m71\u001b[0m\n\u001b[0;31m    X_test_sequences = tokenizer.texts_to_sequences(X_test)   500x500 , 100x100\u001b[0m\n\u001b[0m                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"]}]}]}